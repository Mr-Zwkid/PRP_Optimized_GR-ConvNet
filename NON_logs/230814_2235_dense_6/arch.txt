----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 16, 224, 224]           5,200
       BatchNorm2d-2         [-1, 16, 224, 224]              32
              ReLU-3         [-1, 16, 224, 224]               0
              Conv-4         [-1, 16, 224, 224]               0
            Conv2d-5         [-1, 32, 112, 112]           4,640
       BatchNorm2d-6         [-1, 32, 112, 112]              64
              ReLU-7         [-1, 32, 112, 112]               0
              Conv-8         [-1, 32, 112, 112]               0
            Conv2d-9           [-1, 64, 56, 56]          18,496
      BatchNorm2d-10           [-1, 64, 56, 56]             128
             ReLU-11           [-1, 64, 56, 56]               0
             Conv-12           [-1, 64, 56, 56]               0
      BatchNorm2d-13           [-1, 64, 56, 56]             128
             ReLU-14           [-1, 64, 56, 56]               0
           Conv2d-15           [-1, 16, 56, 56]           1,024
      BatchNorm2d-16           [-1, 16, 56, 56]              32
             ReLU-17           [-1, 16, 56, 56]               0
           Conv2d-18           [-1, 32, 56, 56]           4,608
      Dense_Layer-19           [-1, 96, 56, 56]               0
      BatchNorm2d-20           [-1, 96, 56, 56]             192
             ReLU-21           [-1, 96, 56, 56]               0
           Conv2d-22           [-1, 16, 56, 56]           1,536
      BatchNorm2d-23           [-1, 16, 56, 56]              32
             ReLU-24           [-1, 16, 56, 56]               0
           Conv2d-25           [-1, 32, 56, 56]           4,608
      Dense_Layer-26          [-1, 128, 56, 56]               0
     _Dense_Block-27          [-1, 128, 56, 56]               0
      BatchNorm2d-28          [-1, 128, 56, 56]             256
             ReLU-29          [-1, 128, 56, 56]               0
           Conv2d-30           [-1, 64, 56, 56]           8,256
        AvgPool2d-31           [-1, 64, 28, 28]               0
       Transition-32           [-1, 64, 28, 28]               0
      Dense_Block-33           [-1, 64, 28, 28]               0
      BatchNorm2d-34           [-1, 64, 28, 28]             128
             ReLU-35           [-1, 64, 28, 28]               0
           Conv2d-36           [-1, 16, 28, 28]           1,024
      BatchNorm2d-37           [-1, 16, 28, 28]              32
             ReLU-38           [-1, 16, 28, 28]               0
           Conv2d-39           [-1, 32, 28, 28]           4,608
      Dense_Layer-40           [-1, 96, 28, 28]               0
      BatchNorm2d-41           [-1, 96, 28, 28]             192
             ReLU-42           [-1, 96, 28, 28]               0
           Conv2d-43           [-1, 16, 28, 28]           1,536
      BatchNorm2d-44           [-1, 16, 28, 28]              32
             ReLU-45           [-1, 16, 28, 28]               0
           Conv2d-46           [-1, 32, 28, 28]           4,608
      Dense_Layer-47          [-1, 128, 28, 28]               0
      BatchNorm2d-48          [-1, 128, 28, 28]             256
             ReLU-49          [-1, 128, 28, 28]               0
           Conv2d-50           [-1, 16, 28, 28]           2,048
      BatchNorm2d-51           [-1, 16, 28, 28]              32
             ReLU-52           [-1, 16, 28, 28]               0
           Conv2d-53           [-1, 32, 28, 28]           4,608
      Dense_Layer-54          [-1, 160, 28, 28]               0
     _Dense_Block-55          [-1, 160, 28, 28]               0
      BatchNorm2d-56          [-1, 160, 28, 28]             320
             ReLU-57          [-1, 160, 28, 28]               0
           Conv2d-58           [-1, 64, 28, 28]          10,304
       Transition-59           [-1, 64, 28, 28]               0
      Dense_Block-60           [-1, 64, 28, 28]               0
      BatchNorm2d-61           [-1, 64, 28, 28]             128
             ReLU-62           [-1, 64, 28, 28]               0
           Conv2d-63           [-1, 16, 28, 28]           1,024
      BatchNorm2d-64           [-1, 16, 28, 28]              32
             ReLU-65           [-1, 16, 28, 28]               0
           Conv2d-66           [-1, 32, 28, 28]           4,608
      Dense_Layer-67           [-1, 96, 28, 28]               0
      BatchNorm2d-68           [-1, 96, 28, 28]             192
             ReLU-69           [-1, 96, 28, 28]               0
           Conv2d-70           [-1, 16, 28, 28]           1,536
      BatchNorm2d-71           [-1, 16, 28, 28]              32
             ReLU-72           [-1, 16, 28, 28]               0
           Conv2d-73           [-1, 32, 28, 28]           4,608
      Dense_Layer-74          [-1, 128, 28, 28]               0
      BatchNorm2d-75          [-1, 128, 28, 28]             256
             ReLU-76          [-1, 128, 28, 28]               0
           Conv2d-77           [-1, 16, 28, 28]           2,048
      BatchNorm2d-78           [-1, 16, 28, 28]              32
             ReLU-79           [-1, 16, 28, 28]               0
           Conv2d-80           [-1, 32, 28, 28]           4,608
      Dense_Layer-81          [-1, 160, 28, 28]               0
     _Dense_Block-82          [-1, 160, 28, 28]               0
      BatchNorm2d-83          [-1, 160, 28, 28]             320
             ReLU-84          [-1, 160, 28, 28]               0
           Conv2d-85           [-1, 64, 28, 28]          10,304
       Transition-86           [-1, 64, 28, 28]               0
      Dense_Block-87           [-1, 64, 28, 28]               0
      BatchNorm2d-88           [-1, 64, 28, 28]             128
             ReLU-89           [-1, 64, 28, 28]               0
           Conv2d-90           [-1, 16, 28, 28]           1,024
      BatchNorm2d-91           [-1, 16, 28, 28]              32
             ReLU-92           [-1, 16, 28, 28]               0
           Conv2d-93           [-1, 32, 28, 28]           4,608
      Dense_Layer-94           [-1, 96, 28, 28]               0
      BatchNorm2d-95           [-1, 96, 28, 28]             192
             ReLU-96           [-1, 96, 28, 28]               0
           Conv2d-97           [-1, 16, 28, 28]           1,536
      BatchNorm2d-98           [-1, 16, 28, 28]              32
             ReLU-99           [-1, 16, 28, 28]               0
          Conv2d-100           [-1, 32, 28, 28]           4,608
     Dense_Layer-101          [-1, 128, 28, 28]               0
     BatchNorm2d-102          [-1, 128, 28, 28]             256
            ReLU-103          [-1, 128, 28, 28]               0
          Conv2d-104           [-1, 16, 28, 28]           2,048
     BatchNorm2d-105           [-1, 16, 28, 28]              32
            ReLU-106           [-1, 16, 28, 28]               0
          Conv2d-107           [-1, 32, 28, 28]           4,608
     Dense_Layer-108          [-1, 160, 28, 28]               0
    _Dense_Block-109          [-1, 160, 28, 28]               0
     BatchNorm2d-110          [-1, 160, 28, 28]             320
            ReLU-111          [-1, 160, 28, 28]               0
          Conv2d-112           [-1, 64, 28, 28]          10,304
      Transition-113           [-1, 64, 28, 28]               0
     Dense_Block-114           [-1, 64, 28, 28]               0
 ConvTranspose2d-115           [-1, 32, 56, 56]          32,800
     BatchNorm2d-116           [-1, 32, 56, 56]              64
           Trans-117           [-1, 32, 56, 56]               0
 ConvTranspose2d-118         [-1, 16, 112, 112]           8,208
     BatchNorm2d-119         [-1, 16, 112, 112]              32
           Trans-120         [-1, 16, 112, 112]               0
 ConvTranspose2d-121         [-1, 16, 225, 225]           4,112
     BatchNorm2d-122         [-1, 16, 225, 225]              32
           Trans-123         [-1, 16, 225, 225]               0
 ConvTranspose2d-124         [-1, 16, 225, 225]          20,752
           Trans-125         [-1, 16, 225, 225]               0
         Dropout-126         [-1, 16, 225, 225]               0
          Conv2d-127          [-1, 1, 224, 224]              65
         Dropout-128         [-1, 16, 225, 225]               0
          Conv2d-129          [-1, 1, 224, 224]              65
         Dropout-130         [-1, 16, 225, 225]               0
          Conv2d-131          [-1, 1, 224, 224]              65
         Dropout-132         [-1, 16, 225, 225]               0
          Conv2d-133          [-1, 1, 224, 224]              65
================================================================
Total params: 204,676
Trainable params: 204,676
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.77
Forward/backward pass size (MB): 169.22
Params size (MB): 0.78
Estimated Total Size (MB): 170.76
----------------------------------------------------------------
